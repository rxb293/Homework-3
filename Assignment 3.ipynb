{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "def stopwordremoval(text):\n",
    "    return [word for word in re.split('\\s',text) if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in re.split('\\s',text)]\n",
    "\n",
    "def stemmer(text):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return [porter_stemmer.stem(token) for token in word_tokenize(text)]\n",
    "\n",
    "def removepunc(s):\n",
    "    return re.sub(r'[^\\w\\s]','',s)\n",
    "\n",
    "def joinlist(listoftext):\n",
    "    return ' '.join(listoftext)\n",
    "\n",
    "def tokenindex(text, token):\n",
    "    tokenized = word_tokenize(text)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] == token:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Files\n",
    "ceo = pd.read_csv('ceo.csv',encoding = 'cp1252',names = ['column 1','column 2'])\n",
    "companies = pd.read_csv('companies.csv',names = ['text'])\n",
    "percentages = pd.read_csv('percentage.csv',encoding = 'cp1252',names = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate ceo names into one column\n",
    "ceo['column 2'].fillna('blank',inplace= True)\n",
    "ceo['column 1'].fillna('blank',inplace= True)\n",
    "ceo['text'] = None\n",
    "for i in range(len(ceo)):        \n",
    "    ceo['text'][i] =  ceo['column 1'][i] + ' ' + ceo['column 2'][i]\n",
    "for i in range(len(ceo)):\n",
    "    if ceo['column 2'][i] == 'blank':\n",
    "        ceo['text'][i] = ceo['column 1'][i]\n",
    "for i in range(len(ceo)):\n",
    "    if ceo['column 1'][i] == 'blank':\n",
    "        ceo['text'][i] = ceo['column 2'][i]\n",
    "\n",
    "ceo.drop(['column 1','column 2'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize Company names, and identify companies that have names changed dued to lemanitization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "companies['lemma'] = None\n",
    "for i in range(len(companies)):\n",
    "    companies['lemma'][i]=[lemmatizer.lemmatize(token) for token in re.split('\\s',companies['text'][i])]\n",
    "    companies['lemma'][i] = ' '.join(companies['lemma'][i])\n",
    "companies[companies['text']!=companies['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize ceo names, and identify ceo that have names changed dued to lemanitization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ceo['lemma'] = None\n",
    "for i in range(len(ceo)):\n",
    "    ceo['lemma'][i]=[lemmatizer.lemmatize(token) for token in re.split('\\s',ceo['text'][i])]\n",
    "    ceo['lemma'][i] = ' '.join(ceo['lemma'][i])\n",
    "ceo[ceo['text']!=ceo['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Articles into list of articles\n",
    "articles = []\n",
    "\n",
    "for file in os.listdir('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/2013'):\n",
    "    articles.append(open(os.path.join('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/2013',file),'rb').read().decode('utf-8','ignore'))\n",
    "for file in os.listdir('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/2014'):\n",
    "    articles.append(open(os.path.join('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/2014',file),'rb').read().decode('utf-8','ignore'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sent_tokenize: Break Text into sentences\n",
    "sentences = [sent_tokenize(article) for article in articles]\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = []\n",
    "complete_sentences = []\n",
    "count = 0\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        tokenized_sentences.append(re.split('\\s',sentences[i][j]))\n",
    "        complete_sentences.append(sentences[i][j])\n",
    "df_CS = pd.DataFrame(complete_sentences,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Training data with 150000 sample sentences\n",
    "df_analyze = df_CS.head(150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords and Lemmatize Documents or Stem Documents\n",
    "df_analyze['RemovedStopwords'] = df_analyze['text'].apply(stopwordremoval).apply(joinlist)\n",
    "df_analyze['Lemmatized'] = df_analyze['RemovedStopwords'].apply(lemmatization).apply(joinlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Copies for use in CEO and company name extraction\n",
    "df_analyze_ceo = df_analyze.copy(deep=True)\n",
    "df_analyze_companies = df_analyze.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find all sentences with CEO that have name length >1 (Mark Zuckerberg and not Zuckerberg)\n",
    "df_analyze_ceo['ceo_labels'] = 0\n",
    "df_analyze_ceo['names'] = None\n",
    "for i in range(len(ceo)):\n",
    "    pat = re.compile(r'\\b' + ceo['text'][i]+ r'\\b')\n",
    "    for j in range(len(df_analyze_ceo)):\n",
    "        match = re.search(pat,df_analyze_ceo['text'][j])\n",
    "        if (match != None):\n",
    "                if ((len(word_tokenize(ceo['text'][i])) > 1)):\n",
    "                    df_analyze_ceo.iloc[j, df_analyze_ceo.columns.get_loc('ceo_labels')] = 1\n",
    "                    df_analyze_ceo.iloc[j, df_analyze_ceo.columns.get_loc('names')]  = match.group()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_ceo_ML_positive = df_analyze_ceo[df_analyze_ceo['ceo_labels']==1]\n",
    "df_analyze_ceo_ML_positive.drop(['text','RemovedStopwords'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define function that find names from the entire corpus\n",
    "\n",
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    return (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run this. NAME Extraction: saved in export_list\n",
    "names = []\n",
    "for i in len(articles):\n",
    "    names.append(get_person_names(articles[i]))\n",
    "names = list(dict.fromkeys(names))\n",
    "# Covert list of list into flat list\n",
    "flat_list_names = [item for sublist in names for item in sublist]\n",
    "# Remove Duplicates\n",
    "flat_list_names = list(dict.fromkeys(flat_list_names))\n",
    "# Remove ceo names\n",
    "not_ceo_names = [x for x in flat_list_names if x not in ceo['text'].tolist()]\n",
    "# Export list for manipulation in excel\n",
    "export_list = pd.DataFrame(not_ceo_names,columns=['text'])\n",
    "export_list.to_csv('export_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start here\n",
    "# Filtered to names that are athletes, politicians, and economists\n",
    "non_ceo = pd.read_csv('non_ceo.csv')\n",
    "non_ceo = non_ceo[non_ceo['yes']==1]\n",
    "non_ceo.drop(['yes'],axis = 1,inplace = True)\n",
    "# Construct Negative Samples for CEOs\n",
    "df_analyze_ceo_negative = df_analyze.copy(deep=True)\n",
    "non_ceo.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sentences with nonCEO that have name length >1\n",
    "df_analyze_ceo_negative['ceo_labels'] = 1\n",
    "df_analyze_ceo_negative['names'] = None\n",
    "for i in range(len(non_ceo)):\n",
    "    pat = re.compile(r'\\b' + non_ceo['text'][i]+ r'\\b')\n",
    "    for j in range(len(df_analyze_ceo_negative)):\n",
    "        match = re.search(pat,df_analyze_ceo_negative['text'][j])\n",
    "        if (match != None):\n",
    "                if ((len(word_tokenize(non_ceo['text'][i])) > 1)):\n",
    "                    df_analyze_ceo_negative.iloc[j, df_analyze_ceo_negative.columns.get_loc('ceo_labels')] = 0\n",
    "                    df_analyze_ceo_negative.iloc[j, df_analyze_ceo_negative.columns.get_loc('names')]  = match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Negative Sample Dataframe -> Combine Negative and Positive samples -> Remove punctuations\n",
    "df_analyze_ceo_ML_negative = df_analyze_ceo_negative[df_analyze_ceo_negative['ceo_labels']==0]\n",
    "df_analyze_ceo_ML_negative.drop(['text','RemovedStopwords'],axis = 1, inplace=True)\n",
    "df_analyze_ceo_ML  = pd.concat([df_analyze_ceo_ML_negative,df_analyze_ceo_ML_positive])\n",
    "df_analyze_ceo_ML.reset_index(inplace=True,drop= True)\n",
    "df_analyze_ceo_ML['Lemmatized'] = df_analyze_ceo_ML['Lemmatized'].apply(lambda x: removepunc(x))\n",
    "df_analyze_ceo_ML['names'] = df_analyze_ceo_ML['names'].apply(lambda x: removepunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_word_before(text,name):\n",
    "    sentence = word_tokenize(text)\n",
    "    indexfirstname = tokenindex(text,word_tokenize(name)[0])\n",
    "    indexlastname = tokenindex(text,word_tokenize(name)[len(word_tokenize(name))-1])\n",
    "    if indexlastname == len(word_tokenize(text)) - 1 and indexfirstname!= None: #If last name is in end of sentence\n",
    "        return sentence[indexfirstname - 1]\n",
    "    elif(indexfirstname != None and indexlastname!= None): #If name is in middle of sentence\n",
    "        return sentence[indexfirstname - 1]\n",
    "    else:\n",
    "        return 'NULL'\n",
    "def grab_word_after(text,name):\n",
    "    sentence = word_tokenize(text)\n",
    "    indexfirstname = tokenindex(text,word_tokenize(name)[0])\n",
    "    indexlastname = tokenindex(text,word_tokenize(name)[len(word_tokenize(name))-1])\n",
    "    if indexlastname == (len(sentence) - 1) and indexfirstname != None: #If last name is in end of sentence\n",
    "        return 'NULL'\n",
    "    elif indexfirstname == 0 and indexlastname != None and indexlastname != 1: #if first name is in beginning of sentence and there are more to the sentence\n",
    "        return sentence[indexlastname+1]\n",
    "    elif(indexfirstname != None and indexlastname!= None): # If name is in middle of sentence\n",
    "        return sentence[indexlastname+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words before and after the names\n",
    "df_analyze_ceo_ML['word_before'] =df_analyze_ceo_ML.apply(lambda x : grab_word_before(x['Lemmatized'],x['names']),axis=1)\n",
    "df_analyze_ceo_ML['word_after'] =df_analyze_ceo_ML.apply(lambda x : grab_word_after(x['Lemmatized'],x['names']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw out unanalyzable data\n",
    "df_analyze_ceo_ML_feed = df_analyze_ceo_ML[(df_analyze_ceo_ML['word_after']!='NULL')]\n",
    "df_analyze_ceo_ML_feed.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Name Features\n",
    "df_analyze_ceo_ML_feed['Capitalized'] = False\n",
    "df_analyze_ceo_ML_feed['lengthofname'] = 0\n",
    "df_analyze_ceo_ML_feed['lengthoftoken'] = 0\n",
    "df_analyze_ceo_ML_feed['nameinbeg'] = False\n",
    "df_analyze_ceo_ML_feed['nameinend'] = False\n",
    "\n",
    "df_analyze_ceo_ML_feed['Capitalized'] = df_analyze_ceo_ML_feed['names'].apply(lambda x: x.istitle())\n",
    "df_analyze_ceo_ML_feed['lengthofname'] = df_analyze_ceo_ML_feed['names'].apply(lambda x: len(x))\n",
    "df_analyze_ceo_ML_feed['lengthoftoken'] = df_analyze_ceo_ML_feed['names'].apply(lambda x: len(word_tokenize(x)))\n",
    "df_analyze_ceo_ML_feed['nameinbeg'] = df_analyze_ceo_ML_feed.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[0])==0,axis = 1)\n",
    "df_analyze_ceo_ML_feed['nameinend'] = df_analyze_ceo_ML_feed.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[len(word_tokenize(x['names']))-1]) == len(word_tokenize(x['Lemmatized']))-1,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Before and After Word Features\n",
    "df_analyze_ceo_ML_feed['beforewordlength'] = 0\n",
    "df_analyze_ceo_ML_feed['beforewordcapitalized'] = False\n",
    "df_analyze_ceo_ML_feed['beforewordcontainnumbers'] = False\n",
    "df_analyze_ceo_ML_feed['beforewordcontainceoindicator'] = False\n",
    "df_analyze_ceo_ML_feed['afterwordlength'] = 0\n",
    "df_analyze_ceo_ML_feed['afterwordcapitalized'] = False\n",
    "df_analyze_ceo_ML_feed['afterwordcontainnumbers'] =False\n",
    "df_analyze_ceo_ML_feed['afterwordcontainceoindicator'] = False\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "def hasceoindicator(inputString):\n",
    "    ceoindicators = ['ceo','chair','chairman','chairwoman','executive','investor','founder','chief']\n",
    "    if inputString.lower() in ceoindicators:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_analyze_ceo_ML_feed['beforewordlength'] = df_analyze_ceo_ML_feed['word_before'].apply(lambda x: len(x))\n",
    "df_analyze_ceo_ML_feed['beforewordcapitalized'] = df_analyze_ceo_ML_feed['word_before'].apply(lambda x: x.istitle())\n",
    "df_analyze_ceo_ML_feed['beforewordcontainnumbers'] = df_analyze_ceo_ML_feed['word_before'].apply(lambda x: hasNumbers(x))\n",
    "df_analyze_ceo_ML_feed['beforewordcontainceoindicator'] = df_analyze_ceo_ML_feed['word_before'].apply(lambda x: hasceoindicator(x))\n",
    "df_analyze_ceo_ML_feed['afterwordlength'] = df_analyze_ceo_ML_feed['word_after'].apply(lambda x: len(x))\n",
    "df_analyze_ceo_ML_feed['afterwordcapitalized'] = df_analyze_ceo_ML_feed['word_after'].apply(lambda x: x.istitle())\n",
    "df_analyze_ceo_ML_feed['afterwordcontainnumbers'] =df_analyze_ceo_ML_feed['word_after'].apply(lambda x: hasNumbers(x))\n",
    "df_analyze_ceo_ML_feed['afterwordcontainceoindicator'] = df_analyze_ceo_ML_feed['word_after'].apply(lambda x: hasceoindicator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sentence Feature\n",
    "def havebusinesswords(inputString):\n",
    "    businesswordindicators = ['yoy','growth','strategy','stock','profit','loss','company','Corporation']\n",
    "    words_re = re.compile(\"|\".join(businesswordindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentencehasceoindicator(inputString):\n",
    "    ceoindicators = ['ceo','executive','investor','founder']\n",
    "    words_re = re.compile(\"|\".join(ceoindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_analyze_ceo_ML_feed['senetencecontainceo'] = df_analyze_ceo_ML_feed['Lemmatized'].apply(lambda x: sentencehasceoindicator(x))\n",
    "df_analyze_ceo_ML_feed['senetencecontainbusinesswords'] = df_analyze_ceo_ML_feed['Lemmatized'].apply(lambda x: havebusinesswords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data that are not analyzable\n",
    "df_analyze_ceo_ML_feed = df_analyze_ceo_ML_feed[df_analyze_ceo_ML_feed['word_after'].isnull() == False]\n",
    "df_analyze_ceo_ML_feed.reset_index(inplace= True, drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words before and after\n",
    "df_analyze_ceo_ML_feed['word_before'] = df_analyze_ceo_ML_feed['word_before'].apply(stemmer).apply(joinlist)\n",
    "df_analyze_ceo_ML_feed['word_after'] = df_analyze_ceo_ML_feed['word_after'].apply(stemmer).apply(joinlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into machine learning algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_analyze_ceo_ML_feed\n",
    "#X = df_analyze_ceo_ML_feed.drop(['Lemmatized','ceo_labels','names','word_before','word_after'],axis = 1)\n",
    "y = df_analyze_ceo_ML_feed['ceo_labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for training dataset\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.reset_index(inplace=True,drop=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encbefore = OneHotEncoder(categories ='auto',handle_unknown='ignore')\n",
    "beforeencode = encbefore.fit_transform(X_train.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore = pd.DataFrame(beforeencode,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore.reset_index(drop=True,inplace=True)\n",
    "X_train = pd.concat([X_train,dfOneHotBefore],axis = 1)\n",
    "\n",
    "encafter = OneHotEncoder(categories='auto',handle_unknown='ignore')\n",
    "afterencode = encafter.fit_transform(X_train.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter = pd.DataFrame(afterencode,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter.reset_index(drop=True,inplace=True)\n",
    "X_train = pd.concat([X_train,dfOneHotAfter],axis = 1)\n",
    "\n",
    "X_train = X_train.drop(['Lemmatized','ceo_labels','names','word_before','word_after'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for testing dataset\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "beforeencode = encbefore.transform(X_test.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore = pd.DataFrame(beforeencode,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore.reset_index(drop=True,inplace=True)\n",
    "X_test = pd.concat([X_test,dfOneHotBefore],axis = 1)\n",
    "\n",
    "afterencode = encafter.transform(X_test.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter = pd.DataFrame(afterencode,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter.reset_index(drop=True,inplace=True)\n",
    "X_test = pd.concat([X_test,dfOneHotAfter],axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['Lemmatized','ceo_labels','names','word_before','word_after'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Naive Bayes\n",
    "# Training score\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Training accuracy: {}\".format(accuracy_score(y_train,y_train_pred)))\n",
    "\n",
    "# Testing score\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: {}\".format(accuracy_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression(class_weight='balanced')\n",
    "logmodel.fit(X_train,y_train)\n",
    "log_predictions = logmodel.predict(X_test)\n",
    "log_train_predictions = clf.predict(X_train)\n",
    "print(\"Training accuracy: {}\".format(accuracy_score(y_train,log_train_predictions)))\n",
    "print('Logistic Reg accuracy: {}'.format(accuracy_score(y_test,log_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible names from complete sentences extracted from corpus (super long to run)\n",
    "df_CS['names'] = df_CS['text'].apply(lambda x: get_human_names(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data ran above\n",
    "df_CS = pd.read_csv('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/ALLPOSSIBLENAMES.csv',lineterminator='\\n',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all sentence - name pairs\n",
    "s = df_CS[df_CS['names'].map(lambda d: len(d)) > 0]\n",
    "s.reset_index(drop = True,inplace =True)\n",
    "res = s.set_index(['text'])['names'].apply(pd.Series).stack()\n",
    "res = res.reset_index()\n",
    "res.drop(['level_1'],axis = 1,inplace = True)\n",
    "res.columns = ['text','names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords and Lemmatize Documents and remove punctuation\n",
    "res['RemovedStopwords'] = res['text'].apply(stopwordremoval).apply(joinlist)\n",
    "res['Lemmatized'] = res['RemovedStopwords'].apply(lemmatization).apply(joinlist)\n",
    "res['Lemmatized'] = res['Lemmatized'].apply(lambda x: removepunc(x))\n",
    "res['names'] = res['names'].apply(lambda x: removepunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab words before and after the names\n",
    "res['word_before'] = res.apply(lambda x : grab_word_before(x['Lemmatized'],x['names']),axis=1)\n",
    "res['word_after'] = res.apply(lambda x : grab_word_after(x['Lemmatized'],x['names']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy of last dataframe, reimport into df_CS_feed for manipulation\n",
    "df_CS_feed = res.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Valid samples for analysis\n",
    "df_CS_feed = df_CS_feed[df_CS_feed['word_after'].isnull() == False]\n",
    "df_CS_feed.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Name Features\n",
    "df_CS_feed['Capitalized'] = False\n",
    "df_CS_feed['lengthofname'] = 0\n",
    "df_CS_feed['lengthoftoken'] = 0\n",
    "df_CS_feed['nameinbeg'] = False\n",
    "df_CS_feed['nameinend'] = False\n",
    "\n",
    "df_CS_feed['Capitalized'] = df_CS_feed['names'].apply(lambda x: x.istitle())\n",
    "df_CS_feed['lengthofname'] = df_CS_feed['names'].apply(lambda x: len(x))\n",
    "df_CS_feed['lengthoftoken'] = df_CS_feed['names'].apply(lambda x: len(word_tokenize(x)))\n",
    "df_CS_feed['nameinbeg'] = df_CS_feed.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[0])==0,axis = 1)\n",
    "df_CS_feed['nameinend'] = df_CS_feed.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[len(word_tokenize(x['names']))-1]) == len(word_tokenize(x['Lemmatized']))-1,axis = 1)\n",
    "\n",
    "# Extract Before and After Word Features\n",
    "df_CS_feed['beforewordlength'] = 0\n",
    "df_CS_feed['beforewordcapitalized'] = False\n",
    "df_CS_feed['beforewordcontainnumbers'] = False\n",
    "df_CS_feed['beforewordcontainceoindicator'] = False\n",
    "df_CS_feed['afterwordlength'] = 0\n",
    "df_CS_feed['afterwordcapitalized'] = False\n",
    "df_CS_feed['afterwordcontainnumbers'] =False\n",
    "df_CS_feed['afterwordcontainceoindicator'] = False\n",
    "df_CS_feed['beforewordlength'] = df_CS_feed['word_before'].apply(lambda x: len(x))\n",
    "df_CS_feed['beforewordcapitalized'] = df_CS_feed['word_before'].apply(lambda x: x.istitle())\n",
    "df_CS_feed['beforewordcontainnumbers'] = df_CS_feed['word_before'].apply(lambda x: hasNumbers(x))\n",
    "df_CS_feed['beforewordcontainceoindicator'] = df_CS_feed['word_before'].apply(lambda x: hasceoindicator(x))\n",
    "df_CS_feed['afterwordlength'] = df_CS_feed['word_after'].apply(lambda x: len(x))\n",
    "df_CS_feed['afterwordcapitalized'] = df_CS_feed['word_after'].apply(lambda x: x.istitle())\n",
    "df_CS_feed['afterwordcontainnumbers'] =df_CS_feed['word_after'].apply(lambda x: hasNumbers(x))\n",
    "df_CS_feed['afterwordcontainceoindicator'] = df_CS_feed['word_after'].apply(lambda x: hasceoindicator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sentence Feature\n",
    "def havebusinesswords(inputString):\n",
    "    businesswordindicators = ['yoy','growth','strategy','stock','profit','loss','company','Corporation']\n",
    "    words_re = re.compile(\"|\".join(businesswordindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentencehasceoindicator(inputString):\n",
    "    ceoindicators = ['ceo','executive','investor','founder']\n",
    "    words_re = re.compile(\"|\".join(ceoindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_CS_feed['senetencecontainceo'] = df_CS_feed['Lemmatized'].apply(lambda x: sentencehasceoindicator(x))\n",
    "df_CS_feed['senetencecontainbusinesswords'] = df_CS_feed['Lemmatized'].apply(lambda x: havebusinesswords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words before and after to feed in getdummy\n",
    "df_CS_feed['word_before'] = df_CS_feed['word_before'].apply(stemmer).apply(joinlist)\n",
    "df_CS_feed['word_after'] = df_CS_feed['word_after'].apply(stemmer).apply(joinlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for testing dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "beforeencode = encbefore.transform(df_CS_feed.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore = pd.DataFrame(beforeencode,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore.reset_index(drop=True,inplace=True)\n",
    "df_CS_feed = pd.concat([df_CS_feed,dfOneHotBefore],axis = 1)\n",
    "\n",
    "afterencode = encafter.transform(df_CS_feed.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter = pd.DataFrame(afterencode,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter.reset_index(drop=True,inplace=True)\n",
    "df_CS_feed = pd.concat([df_CS_feed,dfOneHotAfter],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into machine learning algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_CS_feed.drop(['text','RemovedStopwords','Lemmatized','names','word_before','word_after'],axis = 1)\n",
    "\n",
    "y_predict = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_predict_df = pd.DataFrame(y_predict,columns =['ceo_labels'])\n",
    "df_final_ceo = pd.concat([df_CS_feed,y_predict_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ceo extraction files\n",
    "df_final_ceo[df_final_ceo['ceo_labels']==1][['text','names','ceo_labels']].to_csv('CEO_Extraction2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sentences with Companies           \n",
    "df_analyze_companies['companies_labels'] = 0\n",
    "df_analyze_companies['names'] = None\n",
    "for i in range(len(companies)):\n",
    "    pat = re.compile(r'\\b' + companies['text'][i]+ r'\\b')\n",
    "    for j in range(len(df_analyze_companies)):\n",
    "        match = re.search(pat,df_analyze_companies['text'][j])\n",
    "        if (match != None):\n",
    "                if ((len(word_tokenize(companies['text'][i])) > 1)):\n",
    "                    df_analyze_companies.iloc[j, df_analyze_companies.columns.get_loc('companies_labels')] = 1\n",
    "                    df_analyze_companies.iloc[j, df_analyze_companies.columns.get_loc('names')]  = match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it into new dataframe and keep only lemmatized text\n",
    "df_analyze_companies_ML_positive = df_analyze_companies[df_analyze_companies['companies_labels']==1]\n",
    "df_analyze_companies_ML_positive.drop(['text','RemovedStopwords'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import non_companies names (Organization that have the following words: University, Province, State, Foundation, Tower, Federation, zoo, School, Association, World, Institute, Institution)\n",
    "non_companies = pd.read_csv('non_companies.csv')\n",
    "non_companies = non_companies[non_companies['yes']==1]\n",
    "non_companies.drop(['yes'],axis = 1,inplace = True)\n",
    "non_companies.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML_negative = df_analyze.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML_negative['companies_labels'] = 1\n",
    "df_analyze_companies_ML_negative['names'] = None\n",
    "for i in range(len(non_companies)):\n",
    "    pat = re.compile(r'\\b' + non_companies['text'][i]+ r'\\b')\n",
    "    for j in range(len(df_analyze_companies_ML_negative)):\n",
    "        match = re.search(pat,df_analyze_companies_ML_negative['text'][j])\n",
    "        if (match != None):\n",
    "                if ((len(word_tokenize(non_companies['text'][i])) > 1)):\n",
    "                    df_analyze_companies_ML_negative.iloc[j, df_analyze_companies_ML_negative.columns.get_loc('companies_labels')] = 0\n",
    "                    df_analyze_companies_ML_negative.iloc[j, df_analyze_companies_ML_negative.columns.get_loc('names')]  = match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML_negative = df_analyze_companies_ML_negative[df_analyze_companies_ML_negative['companies_labels']==0]\n",
    "df_analyze_companies_ML_negative.drop(['text','RemovedStopwords'],axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append additional negative samples using positive samples in ceo\n",
    "df_analyze_ceo_companies_additionalnegative = df_analyze_ceo_ML_positive[['Lemmatized','ceo_labels','names']].copy(deep=True)\n",
    "df_analyze_ceo_companies_additionalnegative.columns = ['Lemmatized','companies_labels','names']\n",
    "df_analyze_ceo_companies_additionalnegative['companies_labels'] = 0 \n",
    "df_analyze_companies_ML_negative = df_analyze_companies_ML_negative.append(df_analyze_ceo_companies_additionalnegative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML  = pd.concat([df_analyze_companies_ML_negative,df_analyze_companies_ML_positive])\n",
    "df_analyze_companies_ML.reset_index(inplace=True,drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation for easier analyzation\n",
    "df_analyze_companies_ML['Lemmatized'] = df_analyze_companies_ML['Lemmatized'].apply(lambda x: removepunc(x))\n",
    "df_analyze_companies_ML['names'] = df_analyze_companies_ML['names'].apply(lambda x: removepunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML['word_before'] = df_analyze_companies_ML.apply(lambda x : grab_word_before(x['Lemmatized'],x['names']),axis=1)\n",
    "df_analyze_companies_ML['word_after'] = df_analyze_companies_ML.apply(lambda x : grab_word_after(x['Lemmatized'],x['names']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML['Capitalized'] = False\n",
    "df_analyze_companies_ML['lengthofname'] = 0\n",
    "df_analyze_companies_ML['lengthoftoken'] = 0\n",
    "df_analyze_companies_ML['nameinbeg'] = False\n",
    "df_analyze_companies_ML['nameinend'] = False\n",
    "df_analyze_companies_ML['wordcontaincompanyindicator'] = False\n",
    "def hascompanyindicator(inputString):\n",
    "    companyindicators = ['Inc','Corp','Corporation','Bank','LLC','Group','Ltd','Ventures','Capital','Partners','Company','Holdings']\n",
    "    words_re = re.compile(\"|\".join(companyindicators))\n",
    "    if words_re.search(inputString)!=None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_analyze_companies_ML['Capitalized'] = df_analyze_companies_ML['names'].apply(lambda x: x.istitle())\n",
    "df_analyze_companies_ML['lengthofname'] = df_analyze_companies_ML['names'].apply(lambda x: len(x))\n",
    "df_analyze_companies_ML['lengthoftoken'] = df_analyze_companies_ML['names'].apply(lambda x: len(word_tokenize(x)))\n",
    "df_analyze_companies_ML['nameinbeg'] = df_analyze_companies_ML.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[0])==0,axis = 1)\n",
    "df_analyze_companies_ML['nameinend'] = df_analyze_companies_ML.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[len(word_tokenize(x['names']))-1]) == len(word_tokenize(x['Lemmatized']))-1,axis = 1)\n",
    "df_analyze_companies_ML['wordcontaincompanyindicator'] = df_analyze_companies_ML['names'].apply(lambda x: hascompanyindicator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to analyzable data\n",
    "df_analyze_companies_ML = df_analyze_companies_ML[df_analyze_companies_ML['word_after'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML['beforewordlength'] = 0\n",
    "df_analyze_companies_ML['beforewordcapitalized'] = False\n",
    "df_analyze_companies_ML['beforewordcontainnumbers'] = False\n",
    "df_analyze_companies_ML['afterwordlength'] = 0\n",
    "df_analyze_companies_ML['afterwordcapitalized'] = False\n",
    "df_analyze_companies_ML['afterwordcontainnumbers'] =False\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "df_analyze_companies_ML['beforewordlength'] = df_analyze_companies_ML['word_before'].apply(lambda x: len(x))\n",
    "df_analyze_companies_ML['beforewordcapitalized'] = df_analyze_companies_ML['word_before'].apply(lambda x: x.istitle())\n",
    "df_analyze_companies_ML['beforewordcontainnumbers'] = df_analyze_companies_ML['word_before'].apply(lambda x: hasNumbers(x))\n",
    "df_analyze_companies_ML['afterwordlength'] = df_analyze_companies_ML['word_after'].apply(lambda x: len(x))\n",
    "df_analyze_companies_ML['afterwordcapitalized'] = df_analyze_companies_ML['word_after'].apply(lambda x: x.istitle())\n",
    "df_analyze_companies_ML['afterwordcontainnumbers'] = df_analyze_companies_ML['word_after'].apply(lambda x: hasNumbers(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sentence Feature\n",
    "def havebusinesswords(inputString):\n",
    "    businesswordindicators = ['yoy','growth','strategy','stock','profit','loss','company','Corporation']\n",
    "    words_re = re.compile(\"|\".join(businesswordindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentencehasceoindicator(inputString):\n",
    "    ceoindicators = ['ceo','executive','investor','founder']\n",
    "    words_re = re.compile(\"|\".join(ceoindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_analyze_companies_ML['senetencecontainceo'] = df_analyze_companies_ML['Lemmatized'].apply(lambda x: sentencehasceoindicator(x))\n",
    "df_analyze_companies_ML['senetencecontainbusinesswords'] = df_analyze_companies_ML['Lemmatized'].apply(lambda x: havebusinesswords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words before and after\n",
    "df_analyze_companies_ML['word_before'] = df_analyze_companies_ML['word_before'].apply(stemmer).apply(joinlist)\n",
    "df_analyze_companies_ML['word_after'] = df_analyze_companies_ML['word_after'].apply(stemmer).apply(joinlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('X_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyze_companies_ML = pd.read_csv('df_analyze_companies_ML.csv',index_col=0)\n",
    "df_analyze_companies_ML = df_analyze_companies_ML[df_analyze_companies_ML['word_after'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into machine learning algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_analyze_companies_ML\n",
    "y = df_analyze_companies_ML['companies_labels']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for training dataset\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.reset_index(inplace=True,drop=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encbefore = OneHotEncoder(categories ='auto',handle_unknown='ignore')\n",
    "beforeencode = encbefore.fit_transform(X_train.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore = pd.DataFrame(beforeencode,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore.reset_index(drop=True,inplace=True)\n",
    "X_train = pd.concat([X_train,dfOneHotBefore],axis = 1)\n",
    "\n",
    "encafter = OneHotEncoder(categories='auto',handle_unknown='ignore')\n",
    "afterencode = encafter.fit_transform(X_train.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter = pd.DataFrame(afterencode,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter.reset_index(drop=True,inplace=True)\n",
    "X_train = pd.concat([X_train,dfOneHotAfter],axis = 1)\n",
    "\n",
    "X_train = X_train.drop(['Lemmatized','companies_labels','names','word_before','word_after'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for testing dataset\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "beforeencode = encbefore.transform(X_test.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore = pd.DataFrame(beforeencode,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore.reset_index(drop=True,inplace=True)\n",
    "X_test = pd.concat([X_test,dfOneHotBefore],axis = 1)\n",
    "\n",
    "afterencode = encafter.transform(X_test.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter = pd.DataFrame(afterencode,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter.reset_index(drop=True,inplace=True)\n",
    "X_test = pd.concat([X_test,dfOneHotAfter],axis = 1)\n",
    "\n",
    "X_test = X_test.drop(['Lemmatized','companies_labels','names','word_before','word_after'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9102085923470092\n",
      "Test accuracy: 0.8262971318079279\n",
      "[[1495  180]\n",
      " [ 359 1069]]\n"
     ]
    }
   ],
   "source": [
    "# # Use Naive Bayes\n",
    "# Training score\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Training accuracy: {}\".format(accuracy_score(y_train,y_train_pred)))\n",
    "\n",
    "# Testing score\n",
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Test accuracy: {}\".format(accuracy_score(y_test,y_test_pred)))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Died from here: read file saved from before\n",
    "df_CS_feed = pd.read_csv('/Users/eric/Documents/Northwestern University/McCormick/2018-2019:2/IEMS 308/Assignment 3/df_CS_feed',delimiter = ',',lineterminator='\\n',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Algorithm on all sentence_name pairs\n",
    "df_CS_feed_companies = df_CS_feed[df_CS_feed.columns[[0,1,2,3,4,5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply name features\n",
    "df_CS_feed_companies['Capitalized'] = False\n",
    "df_CS_feed_companies['lengthofname'] = 0\n",
    "df_CS_feed_companies['lengthoftoken'] = 0\n",
    "df_CS_feed_companies['nameinbeg'] = False\n",
    "df_CS_feed_companies['nameinend'] = False\n",
    "df_CS_feed_companies['wordcontaincompanyindicator'] = False\n",
    "def hascompanyindicator(inputString):\n",
    "    companyindicators = ['Inc','Corp','Corporation','Bank','LLC','Group','Ltd','Ventures','Education','Capital','Partners','Company','Holdings']\n",
    "    words_re = re.compile(\"|\".join(companyindicators))\n",
    "    if words_re.search(inputString)!=None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "df_CS_feed_companies['Capitalized'] = df_CS_feed_companies['names'].apply(lambda x: x.istitle())\n",
    "df_CS_feed_companies['lengthofname'] = df_CS_feed_companies['names'].apply(lambda x: len(x))\n",
    "df_CS_feed_companies['lengthoftoken'] = df_CS_feed_companies['names'].apply(lambda x: len(word_tokenize(x)))\n",
    "df_CS_feed_companies['nameinbeg'] = df_CS_feed_companies.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[0])==0,axis = 1)\n",
    "df_CS_feed_companies['nameinend'] = df_CS_feed_companies.apply(lambda x: tokenindex(x['Lemmatized'], word_tokenize(x['names'])[len(word_tokenize(x['names']))-1]) == len(word_tokenize(x['Lemmatized']))-1,axis = 1)\n",
    "df_CS_feed_companies['wordcontaincompanyindicator'] = df_CS_feed_companies['names'].apply(lambda x: hascompanyindicator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to analyzable data\n",
    "df_CS_feed_companies = df_CS_feed_companies[df_CS_feed_companies['word_after'].isnull()==False]\n",
    "\n",
    "# Extract before and after word features\n",
    "df_CS_feed_companies['beforewordlength'] = 0\n",
    "df_CS_feed_companies['beforewordcapitalized'] = False\n",
    "df_CS_feed_companies['beforewordcontainnumbers'] = False\n",
    "df_CS_feed_companies['afterwordlength'] = 0\n",
    "df_CS_feed_companies['afterwordcapitalized'] = False\n",
    "df_CS_feed_companies['afterwordcontainnumbers'] =False\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "df_CS_feed_companies['beforewordlength'] = df_CS_feed_companies['word_before'].apply(lambda x: len(x))\n",
    "df_CS_feed_companies['beforewordcapitalized'] = df_CS_feed_companies['word_before'].apply(lambda x: x.istitle())\n",
    "df_CS_feed_companies['beforewordcontainnumbers'] = df_CS_feed_companies['word_before'].apply(lambda x: hasNumbers(x))\n",
    "df_CS_feed_companies['afterwordlength'] = df_CS_feed_companies['word_after'].apply(lambda x: len(x))\n",
    "df_CS_feed_companies['afterwordcapitalized'] = df_CS_feed_companies['word_after'].apply(lambda x: x.istitle())\n",
    "df_CS_feed_companies['afterwordcontainnumbers'] = df_CS_feed_companies['word_after'].apply(lambda x: hasNumbers(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Sentence Feature\n",
    "def havebusinesswords(inputString):\n",
    "    businesswordindicators = ['yoy','growth','strategy','stock','profit','loss','company','Corporation']\n",
    "    words_re = re.compile(\"|\".join(businesswordindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def sentencehasceoindicator(inputString):\n",
    "    ceoindicators = ['ceo','executive','investor','founder']\n",
    "    words_re = re.compile(\"|\".join(ceoindicators))\n",
    "    if words_re.search(inputString.lower()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_CS_feed_companies['senetencecontainceo'] = df_CS_feed_companies['Lemmatized'].apply(lambda x: sentencehasceoindicator(x))\n",
    "df_CS_feed_companies['senetencecontainbusinesswords'] = df_CS_feed_companies['Lemmatized'].apply(lambda x: havebusinesswords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words before and after\n",
    "df_CS_feed_companies['word_before'] = df_CS_feed_companies['word_before'].apply(stemmer).apply(joinlist)\n",
    "df_CS_feed_companies['word_after'] = df_CS_feed_companies['word_after'].apply(stemmer).apply(joinlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CS_feed_companies.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode before and after words for testing dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "beforeencode2 = encbefore.transform(df_CS_feed_companies.word_before.values.reshape(-1,1)).toarray()\n",
    "dfOneHotBefore2 = pd.DataFrame(beforeencode2,columns = ['before_'+str(int(i)) for i in range (beforeencode.shape[1])])\n",
    "dfOneHotBefore2.reset_index(drop=True,inplace=True)\n",
    "df_CS_feed_companies = pd.concat([df_CS_feed_companies,dfOneHotBefore2],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "afterencode2 = encafter.transform(df_CS_feed_companies.word_after.values.reshape(-1,1)).toarray()\n",
    "dfOneHotAfter2 = pd.DataFrame(afterencode2,columns = ['after_'+str(int(i)) for i in range (afterencode.shape[1])])\n",
    "dfOneHotAfter2.reset_index(drop=True,inplace=True)\n",
    "df_CS_feed_companies = pd.concat([df_CS_feed_companies,dfOneHotAfter2],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For double check\n",
    "df_CS_feed_companies = df_CS_feed_companies[['text', 'names', 'RemovedStopwords', 'Lemmatized', 'word_before',\n",
    "       'word_after', 'Capitalized', 'lengthofname', 'lengthoftoken',\n",
    "       'nameinbeg', 'nameinend', 'wordcontaincompanyindicator',\n",
    "       'beforewordlength', 'beforewordcapitalized', 'beforewordcontainnumbers',\n",
    "       'afterwordlength', 'afterwordcapitalized', 'afterwordcontainnumbers',\n",
    "       'senetencecontainceo', 'senetencecontainbusinesswords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed data to model\n",
    "X = df_CS_feed_companies.drop(['text','RemovedStopwords','Lemmatized','names','word_before','word_after'],axis = 1)\n",
    "y_predict = clf.predict(X)\n",
    "y_predict_df = pd.DataFrame(y_predict,columns =['companies_labels'])\n",
    "df_final_companies = pd.concat([df_CS_feed_companies,y_predict_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_companies[df_final_companies['companies_labels']==1][['text','names','companies_labels']].to_csv('final_companies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regex tokenizer\n",
    "def findpercentage(text):\n",
    "    pat = re.compile(r'([\\d\\w\\-.])+(\\%|\\s\\%|\\s\\b[Pp]ercent\\b|\\s\\b[Pp]ercentage\\spoint\\b|\\s\\b[Pp]ercentage\\spoints\\b|\\s\\b[Pp]ercentage\\b|\\s\\b[Pp]ercentile\\spoint\\b|\\s\\b[Pp]ercentile\\spoints\\b)')\n",
    "    listofpercentage = []\n",
    "    if re.finditer !=None: \n",
    "        for match in re.finditer(pat,text):\n",
    "            listofpercentage.append(match.group(0))\n",
    "    return listofpercentage\n",
    "# tokenize text - remember to convert text to lower case\n",
    "df_CS['percentages'] = df_CS['text'].apply(lambda x: findpercentage(x))\n",
    "# Extract sentence-percentage pair\n",
    "s2 = df_CS[df_CS['percentages'].map(lambda d: len(d)) > 0]\n",
    "s2.reset_index(drop = True,inplace =True)\n",
    "res2 = s2.set_index(['text'])['percentages'].apply(pd.Series).stack()\n",
    "res2 = res2.reset_index()\n",
    "res2.drop(['level_1'],axis = 1,inplace = True)\n",
    "res2.columns = ['text','percentages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.to_csv('Percentages_Extraction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
